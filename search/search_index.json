{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"graph-embeds/","title":"Graph Embeddings for Document Similarity","text":""},{"location":"graph-embeds/#construct-node2vec-embeddings","title":"Construct Node2Vec Embeddings","text":"<p>In our first milestone of the project, we will load the graph into Neo4j and generate node2vec node embeddings using the Graph Data Science (GDS) library. These embeddings are dense feature vectors that represent the structure around each node. In this high-dimensional space, nodes that have similar neighborhood structures are closer together than nodes with different neighborhood structures.</p> <p>Node2vec is one of a family of embeddings that are based on making random walks on graphs. As an NLP practitioner, you are undoubtedly familiar with word2vec. The intuition behind node2vec is similar to word2vec. With word2vec, the idea is to train a network to predict whether a pair of words \u201cbelong\u201d together. The belongingness is learned using training data from naturally occurring sentences in text, by finding pairs of words that co-occur within a fixed-size window. With node2vec and its predecessors, random walk-based embedding strategies such as DeepWalk, one constructs sequences of nodes by executing random walks from each node. These sequences are used to find nodes that co-occur within a fixed window size, and a network is trained to predict that these nodes are similar.</p> <p>Early random walk embedding schemes such as DeepWalk treated each relationship as identical, and the random walks generated were truly random. Node2vec, on the other hand, requires two additional constraints that influence the randomness of the walk. These two parameters are the return factor and the in-out parameter. The return factor determines how often a walk returns to its previously visited node, and the in-out parameter determines whether the walk moves away from the source node or gets closer.</p> <ol> <li>Load a graph into Neo4j. Stop the Neo4j server if it is running. Use the neo4j-admin import command to load the provided data files into Neo4j:</li> <li><code>$NEO4J_HOME/bin/neo4j-admin import --database=av2-graph --nodes=/path/to/stat_nodes.csv --relationships=/path/to/stat_edges.csv</code></li> <li>Restart the server. </li> <li>Connect to Neo4j from Python. We use py2neo as our Python interface and interact with Neo4j from Python over the Jupyter notebook using it. py2neo is a lightweight wrapper which does not require any change in the Cypher query being sent, and thus the queries can be used without change directly on the Neo4j web interface as well. </li> <li>Create a virtual graph for GDS. GDS requires all its algorithms to run within a virtual subgraph. The origin of the requirement seems to have been the assumption that algorithms would need to run on a subset of the data. Hence running algorithms within a subgraph makes sense for performance reasons. In our case, we will run the GDS algorithms against our entire graph. But because of the GDS requirement, we will need to create a virtual subgraph that contains the entire graph. For more on virtual graphs, read the neo4j documentation on creating graphs. </li> <li>Generate node2vec vectors. In node2vec you execute fixed-length random walks from each node in the graph. The intuition is that nodes in each walk will depend on the connectivity between them, hence the walks will incorporate the connectivity information.</li> <li>The node2vec family of random walk-based algorithms is influenced to some extent by the word2vec algorithm. You can imagine words in your vocabulary as being a graph, where the edge weight between words w1 and w2 would be determined by the probability that w2 follows w1. These probabilities can be derived by analyzing a large enough corpus of text. With node2vec, we start with a graph and synthesize \u201csentences\u201d by executing the random walks.</li> <li>The analogy described above is almost 100% applicable to a predecessor algorithm to node2vec called DeepWalk. DeepWalk is an example of a first order biased random walk algorithm, meaning that the probability of a node being in a random walk is dependent only on its current state. The node2vec algorithm improves upon DeepWalk by being a second order biased random walk algorithm, meaning that the probability of a node being in a walk is dependent not only on its current state but also on its previous state.</li> <li>The second order bias is controlled by two parameters\u2014the returnFactor (p) and inOutFactor (q).</li> <li>The returnFactor governs the probability of backtracking and revisiting a previously visited node in the walk. Values lower than 1 encourage backtracking and values higher than 1 discourage it.</li> <li>The inOutFactor determines whether the walk will move \u201coutward\u201d or \u201cinward.\u201d Setting inOutFactor to values greater than 1 encourages it to choose nodes closer to the previous node, that is, it encourages breadth-first search (BFS). On the other hand, setting it to values higher than 1 encourages the walk to choose nodes further away from the previous node, that is, it encourages depth-first search (DFS).</li> <li>The inOutFactor can be used to tune the behavior of node2vec embeddings to either prefer \u201chomophily\u201d or community-based similarity (for values &lt; 1) or \u201cstructural\u201d or connection-based similarity (for values &gt; 1). This is somewhat analogous to syntactic and semantic similarity in NLP.</li> <li>Read the node2vec documentation to learn how to call the GDS library. We want to invoke the GDS algorithm to compute the node2vec embeddings and write them back into the database under the node property node2vec_vec key.</li> <li> <p>We want to focus on community-based similarity as far as possible, so we use the following parameters for node2vec. You should also experiment with other parameters and observe their effects on the visualizations to get an intuition for how node2vec works.     <pre><code>embeddingDimension: 128\nwalksPerNode: 10\nwalkLength: 80\nwindowSize: 3\ninOutFactor: 0.5\nreturnFactor: 0.5\nconcurrency: 1\niterations: 25\nwriteProperty: \u2018node2vec_vec\u2019\n</code></pre></p> </li> <li> <p>Verify embeddings have been written out. Verify that the node property node2vec_vec exists and is populated for a random document (say doc_id == \"math/0504058\"). </p> </li> <li>Dump out the content of the database. Call poc.export.json.all to dump the database to disk in JSON format.</li> </ol> <p>In this liveProject, we start with a new data source, the citation network. This is a set of document nodes and edges, and can therefore be modeled as a graph in Neo4j. In this milestone, we use the node2vec algorithm from the Neo4j Graph Data Science (GDS) to generate node2vec embeddings for each document. These embeddings are based only on the citation connections and thus represent a different approach to the document corpus than our previous embeddings. In addition, while the previous liveProject started from document embeddings and resulted in a graph, this liveProject starts with a graph and results in embeddings.</p>"},{"location":"graph-embeds/#visualize-citation-embeddings","title":"Visualize Citation Embeddings","text":"<ol> <li>Extract document IDs, vectors, and labels.  The neo4j-dump.json file that we extracted in the previous milestone of this liveProject contains all the information about the Neo4j database. The file is in JSON-L format, which means that each node and relationship record is on its own line as a well-formed JSON structure. The data we need for our current milestone is the document ID, the document vector generated by node2vec, and the category label. They can be found in the JSON at the following locations.</li> <li>doc_id: data'properties'</li> <li>vector: data'properties'</li> <li>label: data'properties'</li> <li>Extract these variables for each node into rhw Python list variables docids, vectors, and labels.</li> <li>At the end of this step each list should have 50426 elements in it.</li> <li>When adding the vector to the list vectors, convert it to a NumPy array first by wrapping the list vector in a np.array(vector). That way the vectors list is a list of numpy.array objects.</li> <li>Use UMAP for dimensionality reduction.  Convert the list vectors of numpy.array objects to a matrix. This is done by wrapping the list in another np.array(vectors).</li> <li>Check that you now have a matrix of size (50426, 128), since each vector has 128 elements.    N- ext convert the labels to a set of numeric IDs. This can be done by creating a dictionary of category strings to label id and then looking up the ID from the label to create a new variable label_ids.</li> <li>Create a UMAP mapper. It is worth experimenting with a few hyperparameters for mapper construction based on information in the UMAP parameters page and seeing what the resulting visualization is like and whether it aligns with what we believe the data should look like.</li> <li>In our case, we will create the UMAP mapper with the following parameters:<ul> <li>n_neighbors 10</li> <li>min_dist 0.1</li> <li>metric \u201ccosine\u201d</li> </ul> </li> <li>Although we are using UMAP here to reduce the dimension of X to 2D, UMAP allows reducing to other dimensions as well.</li> <li>We then plot the points in 2D using umap.plot.points. The plot indicates two major clusters. Each point represents the node2vec embedding for a document projected by UMAP onto 2D space. Each point has an associated color that indicates its category. Documents of all categories seem to be evenly distributed in both clusters.</li> <li>So it looks like node2vec has discovered a coarser structure in the graph that was not implied by the category labels. This makes intuitive sense since citations reflect acknowledgments of past work, and papers in one area of study are very likely to be dependent on papers in other areas of study.</li> <li>Use clustering for HDBSCAN.  Rather than depend only on visual identification of clusters, we can use a clustering algorithm such as HDBSCAN to discover clusters for us. We will cluster on reduced 2D space generated by UMAP rather than the full 128-dimensional node2vec vector space. This is because it is generally very difficult to find clusters in high dimensions because high dimensional data is sparse. As with UMAP, it is worth trying out different hyperparameter values for HDBSCAN as well. This page describes the various available HDBSCAN hyperparameters. We will use the following HDBSCAN parameters to train the clusterer.</li> <li>min_cluster_size 40</li> <li>To keep the output to not be too dense, we sample 5000 points from our input and plot those as a 2D scatterplot. As with UMAP, HDBSCAN also predicts two major clusters. </li> <li>Examine the composition of the largest clusters.  We have seen that UMAP dimensionality reduction and HDBSCAN clustering both show two major clusters in the data. We also notice that the categories seem to be distributed uniformly across both clusters and that the node2vec embedding does not seem to capture this information.</li> <li>We next examine the composition (in terms of document categories) of documents in each of the two largest clusters. First, we find the two largest clusters by counting the labels returned by clusterer.labels_.</li> <li>Remember that HDBSCAN, like other density-based clusterers, will assign an \u201cunknown\u201d or \u201cother\u201d label to documents it is unable to cluster. The \"-1\" cluster can be quite large and have no discernible structure, so we want to ignore it when we find the two largest clusters.</li> <li>For each of the two clusters, we count the different number of labels and plot a bar chart of label counts (y-axis) versus labels (x-axis).</li> <li>We observe that stat.ML (machine learning) is dominant in both clusters. However, in one cluster other subject areas are more common compared to stat.ML, implying that this cluster may have a more \u201crounded\u201d set of articles than the other. In both cases, machine learning papers seem to be very popular in the field of statistics.</li> </ol>"},{"location":"graph-text/","title":"Convert Text to Graph","text":"<p>There are many ways of converting a text corpus to a graph. If you have access to named entity recognition (NER) tools that can identify entities in the text, such as drugs and diseases in medical text, genes and proteins in biomedical text, and people, locations, and organizations in news reports, then you can treat these entities as nodes of your graph and draw edges between documents with co-occurring instances of these entities.</p> <p>Even without a NER tool, you can often use statistical or heuristics-based tools, such as those used to find frequent or likely n-grams, noun phrases, or keywords to identify phrases that can be used as entities and generate graphs from them in a similar manner to entities.</p> <p>More recently, with the increase in popularity of word embedding approaches, words in documents can be projected from a one-hot encoding in a sparse high dimensional vocabulary space to a dense low dimensional space (generally 80, 100, or 300 dimensions, but we have seen embeddings as high as 768 dimensions) where words with similar neighbors cluster closer together than words that have different neighbors. Such a projection generally results in words with similar meanings clustering together as well.</p> <p>Somewhat surprisingly, it has been observed that word embeddings can be extended to sentence and document embeddings merely by averaging the embedding vectors for all the words in the sentence or document. Like similar words, sentences and documents that are similar in this space also tend to be similar to each other.</p> <p>In this milestone, we will use spaCy to generate document embeddings using our paper titles and abstracts, and then compute the similarity between document vectors using cosine similarity. This will result in a dense graph since the distribution of cosine similarity is likely to be fairly continuous between 0 and 1. So we will determine a threshold for similarity based on the distribution and, with all our documents as nodes in a graph, draw edges between any documents where the similarity between them exceeds this threshold.</p> <ol> <li>Load the spaCy language model. We will represent documents as vectors using word embeddings. The spaCy medium and large provides 300-dimensional word vectors out of the box. Documents are represented by vectors that are the average of their word vectors. In our project, we will use the medium English language model (en_core_web_md).</li> <li>Extract document vectors. We loop through the abstracts file provided to us, concatenating the title and text for each abstract. Concatenation involves terminating the title with a period and appending the abstract text to the end, essentially treating the title as the first sentence of the resulting text block representing the abstract, and the abstract as the rest. The concatenated text is passed to the spaCy language model. SpaCy will automatically return the average embedding vector described above for each text block.</li> <li>Construct a dense document matrix. The document vectors are composed into a document matrix of size (N, 300), where N represents the number of documents in the collection.</li> <li>Construct the document similarity matrix. The document similarity matrix is another NumPy matrix, constructed by computing the dot product of the document matrix and its transpose.</li> <li>Determine similarity threshold. The resulting similarity matrix is dense since the document vectors are dense as well. If we built a graph out of this, it would be extremely dense and not provide us with meaningful insights. So we need to establish a similarity threshold above which we will consider a document pair to be related, and below which they would be considered unrelated.</li> <li>We do this by sampling from the similarity matrix and building a histogram of scores. We determine the threshold by inspecting the histogram. The objective is to build an adjacency matrix that represents a graph that is sparse enough to provide a meaningful structure. That is, we connect two documents only if there is a very high similarity.</li> <li>Unfortunately, this is somewhat subjective and you might need a few attempts with different thresholds in order to get a meaningful threshold.</li> <li>Building the histogram against the full document set is quite compute-intensive, so to enable quick development cycles, we sample about 2% of the data (1000 documents) randomly and generate the histogram from it.</li> <li>A good rule of thumb might be to set the threshold to only include edges with weights that are in the top 1\u20135% of the population.</li> <li>Create an adjacency matrix. Remember to set the diagonal to zero. A document is most similar to itself, therefore diagonal elements would be highest, but we are not interested in this relation for our graph. Update the similarity matrix such that any value above the threshold is 1 and any values below it are 0.</li> <li>Save the adjacency matrix . Remember to save the adjacency matrix representing the document graph for the next milestone. We will need the adjacency matrix and a pointer from each row position back to the document ID.</li> </ol> <p>The approach outlined above allows us to leverage document embeddings as features to build an adjacency matrix across the entire corpus.</p>"},{"location":"graph-text/#build-and-explore-a-graph","title":"Build and Explore a Graph","text":"<p>Workflow</p> <ol> <li>Install Neo4j Server. Download the latest Neo4j Server from the Neo4j Download Center. </li> <li>Install Neo4j Server on your disk using instructions for your specific operating system.</li> <li>Recent versions need the Java Development Kit (JDK) 11.x or newer installed as a prerequisite. Follow the instructions appropriate to your operating system to install the JDK.</li> <li>Start the Neo4j server. The directory under which you installed Neo4j will contain a folder with a name starting with <code>neo4j-community-</code>. This is your NEO4J_HOME. To start the Neo4j server at your console, run <code>$NEO4J_HOME/bin/neo4j console</code> at the command line. You should see the server start up and eventually say that its remote interface is listening on port 7474.</li> <li>Navigate to the Neo4j web interface on your browser\u2014navigate to http://localhost:7474. You will be prompted for the Neo4j user and system password (they are <code>neo4j</code> and <code>neo4j</code>, respectively). Once you enter them, you will be prompted to change your password. Choose a new password for user neo4j.</li> <li> <p>You should now be able to enter commands to interact with Neo4j. As a test, enter <code>SHOW DATABASES</code>; in the browser prompt and hit Control-Enter (or Command-Enter for Mac OS users). You should see the databases neo4j and system.</p> </li> <li> <p>Configure the Neo4j server. At this point, our Neo4j server can serve results from user-supplied Cypher queries, but we also want to install the Graph Data Science (GDS) and Awesome Procedures on Cypher (APOC) plugins.</p> </li> <li>To install the GDS plugin, first stop the Neo4j server (Control-C if you used neo4j console to start or <code>$NEO4J_HOME/bin/neo4j stop</code> if you used neo4j start to start the server. </li> <li>Download the plugin from the Neo4j Download Center and copy the JAR file to $NEO4J_PLUGINS/plugins/. </li> <li>To install the APOC plugin, copy the APOC JAR file from $NEO4J_HOME/labs to $NEO4J_HOME/plugins. </li> <li> <p>Update the Neo4j configuration file at $NEO4J_HOME/conf/neo4j.conf and set the following properties:</p> <ul> <li><code>apoc.export.file.enabled</code> to true </li> <li><code>apoc.import.file.enabled</code> to true</li> <li><code>apoc.import.file.use_neo4j_config</code> to false</li> <li><code>dbms.security.procedures.unrestricted</code> to apoc.,gds.</li> <li><code>dbms.memory.heap.initial_size</code> to 8G if you can afford it; otherwise set it to around 50\u201375% of your available RAM</li> <li><code>dbms.memory.heap.max_size</code> to 8G if you can afford it; otherwise set it to around 50\u201375% of your available RAM</li> </ul> </li> <li> <p>Create a new database. This step is not strictly necessary, but in general it is safer to isolate application data from system data, so we will create a separate database to store our graph. That way, if there are issues with the data, we can safely issue a DROP DATABASE command on our database. Unfortunately, Neo4j Community Server doesn\u2019t provide a way to create one from the web browser, but there is a simple hack to do this. In <code>$NEO4J_HOME/conf/neo4j.conf</code>, uncomment the property <code>dbms.default_database</code> and set it to <code>av-graph</code> (that\u2019s the name we give our database).'</p> </li> <li>Restart the Neo4j server. You should see av-graph as a selection option on the left-hand navigation panel of the Neo4j web interface.</li> <li>Create node.csv and edge.csv files. We will convert the adjacency matrix we created in the previous milestone into a pair of CSV files nodes.csv and edges.csv. The nodes.csv file contains information about documents\u2014the ID along with metadata such as title and category. The edges.csv file contains information about the links between similar documents in the graph. The header of either file contains important metadata that communicates the graph structure to Neo4j.</li> <li>Create the graph. To create the graph, convert the adjacency matrix we created in the previous milestone into a node list and an edge list. Refer to the instructions under the section Create Input files for neo4j-admin on how to do this. The output of this step should be a pair of files nodes.csv and edges.tsv.</li> <li>Stop the Neo4j server by using the command $NEO4J_HOME/bin/neo4j stop.</li> <li>Run the neo4j-admin command to load the files into Neo4j: $NEO4J_HOME/bin/neo4j-admin import --database=av-graph --nodes=nodes.csv --relationships=edges.csv.</li> <li>Restart Neo4j server using $NEO4J_HOME/bin/neo4j start or $NEO4J_HOME/bin/neo4j console.</li> <li>Run exploratory commands. Install the py2neo library using pip install py2neo. We will use this library to connect to Neo4j and issue Cypher queries. Go back to the web interface and examine the graph using some Cypher queries to count the number of nodes and edges in the graph. You can do this either from the Neo4j web interface at http://localhost:7474/ using pure Cypher, or connect to the server from your Jupyter notebook using py2neo.</li> </ol> <p>Caution</p> <p>Step (2) is outdated. APOC config should be in its own <code>$NEO4J_HOME/conf/apoc.conf</code> file. Step (5) is outdated. See import docs <code>py2neo</code> is EOL; use the official Python driver: <code>neo4j</code> via pip or another package manager</p>"},{"location":"graph-text/#explore-a-graph-using-neo4j","title":"Explore a Graph Using Neo4j","text":"<ol> <li>Connect to Neo4j. We first connect to the Neo4j server through the bolt interface and then do a sanity check to verify that we are connected to the correct database.</li> <li>Create a subgraph for GDS. The algorithms of the GDS library need to run within a virtual subgraph, so we create one with our Article nodes and SIMILAR_TO relationships.</li> <li>Find important nodes. We can look at importance in different ways. Degree centrality is one such way. Since our graph is based on similarity\u2014that is, documents that are very similar to each other have an edge between them\u2014nodes with a high degree of centrality are those that are similar to most other nodes. We use GDS to retrieve the 10 documents in the graph with the highest degree of centrality.</li> <li>Another useful measure of centrality is PageRank. Like degree centrality, the number of edges incident upon a node matters, but in addition, the quality of the nodes on the far end also matters. The quality is determined by the in-links into those nodes, so it\u2019s a recursive metric.</li> <li>Yet another useful measure of importance is betweenness centrality. Nodes with high betweenness centrality act as bridges between clusters of nodes. In the context of scientific articles, such nodes represent papers that deal with multiple subject areas, and often end up being influential.</li> <li>Use community detection. Community detection is similar to clustering for nongraph data. The important difference is that community detection works with edge information alone. In that sense, it serves as an interesting alternative for finding interesting clusters in graph-based data. We cluster our data using two community detection algorithms. </li> <li>Louvain modularity \u2013 Louvain is a popular community detection algorithm. It works by maximizing the modularity of the created cluster, where the modularity quantifies the quality of assignment of nodes to communities compared to a random graph. You can read more about the Louvain algorithm on the GDS Documentation page for Louvain algorithm.</li> <li>Label propagation \u2013 Label propagation is another community detection algorithm that is path based. It starts by propagating labels from node to its neighbors and forming communities based on the labels. You can read about label propagation on its GDS Documentation Page.</li> <li>We then download the community labels predicted by Louvain and label propagation, as well as the categories we had originally, and use them to construct labels.</li> <li>The node vectors (of dimension 300) are then reduced to 2 dimensions using UMAP for visualization. The clusters produced are color-coded using category labels, and predictions from Louvain and label propagation respectively. Clusters produced from Louvain and label propagation have more definitions compared to the original clusters from category labels.</li> <li>Recommend similar articles. The recommendation task is achieved using the Personalized PageRank (PPR) algorithm. PPR is a variation of PageRank where the random surfer gives up and returns to the same neighborhood with a certain probability instead of jumping to a random location.</li> <li>Given a document, we use Cypher to compute a set of nodes in its immediate neighborhood.</li> <li>Using the nodes in the neighborhood, we call PageRank in personalized mode, by specifying the sourceNodes configuration parameter and get back articles that are similar to the source article.</li> </ol>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#environment","title":"Environment","text":"<ul> <li><code>pdm</code></li> <li><code>Taskfile</code></li> </ul> <p>Libraries:</p> <ul> <li><code>matplotlib</code></li> <li><code>networkx</code></li> <li><code>numpy</code></li> <li><code>pandas</code></li> <li><code>spacy</code></li> <li><code>py2neo</code></li> <li><code>umap</code></li> <li><code>hdbscan</code></li> </ul> <p>We'll use the medium English language model with spaCy. Download instructions here or run <code>task install-elm</code> if you have <code>Taskfile</code> installed on your system.</p>"},{"location":"setup/#neo4j-database","title":"Neo4j Database","text":"<p>Download the Neo4j server (Community Edition) from the Neo4j Download Center and install it locally using instructions for your specific operating system. You will need the Java Development Kit (JDK) 11.x or newer. If you don\u2019t have it already, you will need to download and install it as well, as a prerequisite.</p>"},{"location":"setup/#configure-the-neo4j-server","title":"Configure the Neo4j server","text":"<p>Update the $NEO4J_HOME/config/neo4j.cfg file and set the following properties:</p> <pre><code>apoc.export.file.enabled=true\napoc.import.file.use_neo4j_config=false\napoc.import.file.enabled=true\ndbms.security.procedures.unrestricted=apoc.*,gds.*\n# if you can afford it; otherwise set it to around 50\u201375% of your available RAM\ndbms.memory.heap.initial_size=8G\n# if you can afford it; otherwise set it to around 50\u201375% of your available RAM\ndbms.memory.heap.max_size=8G\n</code></pre> <p>Note</p> <p>Those configs look like they may be for an older version of Neo4j. Py2Neo is EOL; use official Neo4j drivers!</p>"},{"location":"setup/#data","title":"Data","text":""}]}